{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama + OpenAI + JavaScript\n",
    "\n",
    "## 1. Specify the model name \n",
    "\n",
    "If you want to use a different model, you can change the `MODEL_NAME` variable below. This variable is used throughout the notebook.\n",
    "\n",
    "You'll also need to run `ollama pull <model_name>` in a terminal to download the model files (`phi3` model has already been downloaded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "const MODEL_NAME = \"phi3\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup the OpenAI client\n",
    "\n",
    "Typically, the OpenAI client is used with [OpenAI API](https://openai.com/api/) or [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/overview) to interact with large language models. However, it can also be used with Ollama as it provides an OpenAI-compatible endpoint at http://localhost:11434/v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { OpenAI } from \"npm:openai\";\n",
    "\n",
    "const openai = new OpenAI({\n",
    "  baseURL: \"http://localhost:11434/v1\",\n",
    "  apiKey: \"__not_needed_by_ollama__\",\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate a chat completion\n",
    "Now we can use the OpenAI SDK to generate a response for a conversation. This request should generate a haiku about cats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Moonlight on her whiskers,\n",
      "A growling belly beckons—pounce! \n",
      "Dreams of mice at dawn.\n"
     ]
    }
   ],
   "source": [
    "const completion = await openai.chat.completions.create({\n",
    "  model: MODEL_NAME,\n",
    "  messages: [{ role: \"user\", content: \"Write a haiku about a hungry cat\" }],\n",
    "});\n",
    "\n",
    "console.log(\"Response:\\n\" + completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompt engineering\n",
    "\n",
    "The first message sent to the language model is called the \"system message\" or \"system prompt\", and it sets the overall instructions for the model. You can provide your own custom system prompt to guide a language model to generate output in a different way. Modify the `SYSTEM_MESSAGE` below to answer like your favorite famous movie/TV character, or get inspiration for other system prompts from [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts?tab=readme-ov-file#prompts).\n",
    "\n",
    "Once you've customized the system message, provide the first user question in the `USER_MESSAGE`.\n",
    "\n",
    "> ***Tip:** The `temperature` parameter controls the randomness of the model's output. Lower values make the model more deterministic and repetitive, while higher values make the model more creative and unpredictable. The default value is `0.5`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " The Force is calm and steady it always remains, even when faced with darkness or light within you too much can be consumed by one’s feelings only if that leads astray from righteousness it does. Peaceful mind keeps us safe in harmony we stay, eh? Yes, my young Padawan is where to focus your energy must go!\n"
     ]
    }
   ],
   "source": [
    "const SYSTEM_MESSAGE = `Act like Yoda from Star Wars.\n",
    "Respond and answer like Yoda using the tone, manner and vocabulary that Yoda would use.\n",
    "Do not write any explanations. Only answer like Yoda.\n",
    "You must know all of the knowledge of Yoda, and nothing more.`;\n",
    "\n",
    "const USER_MESSAGE = `Hi Yoda, how's the force today?`;\n",
    "\n",
    "const completion = await openai.chat.completions.create({\n",
    "  model: MODEL_NAME,\n",
    "  messages: [\n",
    "    { role: \"system\", content: SYSTEM_MESSAGE },\n",
    "    { role: \"user\", content: USER_MESSAGE },\n",
    "  ],\n",
    "  temperature: 0.7,\n",
    "});\n",
    "\n",
    "console.log(\"Response:\\n\" + completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shot examples\n",
    "\n",
    "Another way to guide a language model is to provide \"few shots\", a sequence of example question/answers that demonstrate how it should respond.\n",
    "\n",
    "The example below tries to get a language model to act like a teaching assistant by providing a few examples of questions and answers that a TA might give, and then prompts the model with a question that a student might ask.\n",
    "\n",
    "Try it first, and then modify the `SYSTEM_MESSAGE`, `EXAMPLES`, and `USER_MESSAGE` for a new scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " Which one, known for its gas giants, has more than four planets orbiting it?\n"
     ]
    }
   ],
   "source": [
    "const SYSTEM_MESSAGE = `You are a helpful assistant that helps students with their homework.\n",
    "Instead of providing the full answer, you respond with a hint or a clue.`;\n",
    "\n",
    "const EXAMPLES = [\n",
    "  [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Can you remember the name of the city that is known for the Eiffel Tower?\",\n",
    "  ],\n",
    "  [\n",
    "    \"What is the square root of 144?\",\n",
    "    \"What number multiplied by itself equals 144?\",\n",
    "  ],\n",
    "  [\n",
    "    \"What is the atomic number of oxygen?\",\n",
    "    \"How many protons does an oxygen atom have?\",\n",
    "  ],\n",
    "];\n",
    "\n",
    "const USER_MESSAGE = \"What is the largest planet in our solar system?\";\n",
    "\n",
    "const completion = await openai.chat.completions.create({\n",
    "  model: MODEL_NAME,\n",
    "  messages: [\n",
    "    { role: \"system\", content: SYSTEM_MESSAGE },\n",
    "    { role: \"user\", content: EXAMPLES[0][0] },\n",
    "    { role: \"assistant\", content: EXAMPLES[0][1] },\n",
    "    { role: \"user\", content: EXAMPLES[1][0] },\n",
    "    { role: \"assistant\", content: EXAMPLES[1][1] },\n",
    "    { role: \"user\", content: EXAMPLES[2][0] },\n",
    "    { role: \"assistant\", content: EXAMPLES[2][1] },\n",
    "    { role: \"user\", content: USER_MESSAGE },\n",
    "  ],\n",
    "  temperature: 0.7,\n",
    "});\n",
    "\n",
    "console.log(\"Response:\\n\" + completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval Augmented Generation\n",
    "\n",
    "RAG (*Retrieval Augmented Generation*) is a technique to get a language model to answer questions accurately for a particular domain, by first retrieving relevant information from a knowledge source and then generating a response based on that information.\n",
    "\n",
    "We have provided a local CSV file with data about hybrid cars. The code below reads the CSV file, searches for matches to the user question, and then generates a response based on the information found. Note that this will take longer than any of the previous examples, as it sends more data to the model. If you notice the answer is still not grounded in the data, you can try system engineering or try other models. Generally, RAG is more effective with either larger models or with fine-tuned versions of SLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 matches:\n",
      "| vehicle | year | msrp | acceleration | mpg | class |\n",
      "|--- | --- | --- | --- | --- | ---|\n",
      "| Prius (1st Gen) | 1997 | 24509.74 | 7.46 | 41.26 | Compact |\n",
      "| Prius (2nd Gen) | 2000 | 26832.25 | 7.97 | 45.23 | Compact |\n",
      "| Prius | 2004 | 20355.64 | 9.9 | 46.0 | Midsize |\n",
      "| Prius (3rd Gen) | 2009 | 24641.18 | 9.6 | 47.98 | Compact |\n",
      "| Prius alpha (V) | 2011 | 30588.35 | 10.0 | 72.92 | Midsize |\n",
      "| Prius V | 2011 | 27272.28 | 9.51 | 32.93 | Midsize |\n",
      "| Prius C | 2012 | 19006.62 | 9.35 | 50.0 | Compact |\n",
      "| Prius PHV | 2012 | 32095.61 | 8.82 | 50.0 | Midsize |\n",
      "| Prius C | 2013 | 19080.0 | 8.7 | 50.0 | Compact |\n",
      "| Prius | 2013 | 24200.0 | 10.2 | 50.0 | Midsize |\n",
      "| Prius Plug-in | 2013 | 32000.0 | 9.17 | 50.0 | Midsize |\n",
      "\n",
      "Answer for \"what's the fastest prius\":\n",
      " The fastest Hyundai Ioniq from the provided sources is the Ioniq PHEV with acceleration of 9.17, as it's listed alongside a higher Prius Plug-in in terms of performance metrics but still categorized under \"Midsize\" class and has similar MPG values compared to other midsize hybr0ds from Toyota such as the Prius alpha (V).\n"
     ]
    }
   ],
   "source": [
    "import fs from \"node:fs\";\n",
    "\n",
    "const SYSTEM_MESSAGE = `Answers questions about cars based off a hybrid car data set.\n",
    "Use the sources to answer the questions, if there's no enough data in provided sources say that you don't know.\n",
    "Be brief and straight to the point.`;\n",
    "\n",
    "const USER_MESSAGE = `what's the fastest prius`;\n",
    "\n",
    "// Load CSV data as an array of objects\n",
    "const rows = fs.readFileSync(\"./data/hybrid.csv\", \"utf8\").split(\"\\n\");\n",
    "const columns = rows[0].split(\",\");\n",
    "\n",
    "// Search the data using a very naive search\n",
    "const words = USER_MESSAGE\n",
    "  .toLowerCase()\n",
    "  .replaceAll(/[.?!()'\":,]/g, \"\")\n",
    "  .split(\" \")\n",
    "  .filter((word) => word.length > 2);\n",
    "const matches = rows.slice(1).filter((row) => words.some((word) => row.toLowerCase().includes(word)));\n",
    "\n",
    "// Format as a markdown table, since language models understand markdown\n",
    "const table =\n",
    "  `| ${columns.join(\" | \")} |\\n` +\n",
    "  `|${columns.map(() => \"---\").join(\" | \")}|\\n` +\n",
    "  matches.map((row) => `| ${row.replaceAll(\",\", \" | \")} |\\n`).join(\"\");\n",
    "\n",
    "console.log(`Found ${matches.length} matches:\\n${table}`);\n",
    "\n",
    "// Use the search results to generate a response\n",
    "const completion = await openai.chat.completions.create({\n",
    "  model: MODEL_NAME,\n",
    "  messages: [\n",
    "    { role: \"system\", content: SYSTEM_MESSAGE },\n",
    "    { role: \"user\", content: `${USER_MESSAGE}\\n\\nSOURCES:\\n${table}` },\n",
    "  ],\n",
    "  temperature: 0.7\n",
    "});\n",
    "\n",
    "console.log(`Answer for \"${USER_MESSAGE}\":\\n` + completion.choices[0].message.content);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next steps\n",
    "\n",
    "You can try different models, system prompts, and examples to see how they affect the model's output. You can also try different user messages to see how the model responds to different questions.\n",
    "\n",
    "In the `sample` folder of this repository, you'll also find more examples of how to use generative AI models.\n",
    "\n",
    "To continue on your learning journey, here are some additional resources you might find helpful:\n",
    "\n",
    "- [Generative AI for beginners](https://github.com/microsoft/generative-ai-for-beginners): a complete guide to learn about generative AI concepts and usage.\n",
    "- [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook): hands-on examples for working with the Phi-3 model.\n",
    "- [Build a serverless AI chat with RAG using LangChain.js](https://techcommunity.microsoft.com/t5/apps-on-azure-blog/build-a-serverless-ai-chat-with-rag-using-langchain-js/ba-p/4111041): a next step tutorial to build an AI chatbot using Retrieval-Augmented Generation and LangChain.js."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
